{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network (ESPCN)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Reproducibility Effort\n",
    "\n",
    "In this blog-post, we compile our efforts to reproduce the posited results from table 1 of the paper [Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network](https://arxiv.org/abs/1609.05158) by Shi et al. Building upon the code effort by Jeffrey Yeo (yjn870) [[Github repo](https://github.com/yjn870/ESPCN-pytorch)], we made certain improvements and additionally reproduced the experiments for 4K and video from the paper, which were not a part of Yeo's previous work. We present our reproducibility effort as follows.\n",
    "\n",
    "<img src=\"thumbnails/DL_Teaser.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Super-Resolution\n",
    "\n",
    "In deep learning, especially in computer vision and related research, many researchers have been focusing on the *ill-posed* image **super-resolution** (SR) problem, which involves upscaling a low resolution image into a high resolution space. This technique could be used to restore image quality and could also enhance general image processing. In fields like facial recognition, medical imaging and even satellite imaging, super-resolution has been widely applied. Its broad use-case has allowed it to become one of the most popular topics in Computer Vision. \n",
    "\n",
    "## Why do we need *ESPCN*?\n",
    " \n",
    "In previous SR models like [SRCNN](https://arxiv.org/abs/1501.00092) and [TNRD](https://arxiv.org/pdf/1508.02848), the super-resolution operation was carried out in the high-resolution (HR) space. The sub-optimality and additional computational complexity of this approach motivated the development of **Efficient Sub-Pixel Convolution layer** (ESPCN). Upscaling the resolution of low-resolution (LR) images before the image enhancement step is a major pain-point behind the increase in computational complexity. In convolutional networks, this complexity severly influences the speed of the implementation. Moreover, traditional interpolation methods used in super-resolution (previously) fail to capture additional (crucial) information required to solve this ill-posed problem!\n",
    "\n",
    "Notably, in the proposed ESPCN, feature maps are extracted in the LR space and upscaling of images (from LR to HR) is performed in the final layer of the network. Super-resolving HR data from LR feature-maps in this way greatly increases the efficiency of the SR model as most of the computation is done in the smaller LR space. What's more? in ESPCN, no explicit interpolation filter is used which means that the network is able to implicitly learn the processing necesaary for super-resolution. It is thus able to learn a better mapping from low-resolution image to high-resolution image compared to using a single fixed filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ESPCN with 2 CNN layers and 1 Sub-Pixel Convolution Layer](thumbnails/fig1.png)\n",
    "\n",
    "##### ESPCN with 2 CNN layers and 1 Sub-Pixel Convolution Layer\n",
    "In the proposed architecture, firstly, an *L* layer convolutional neural network is directly applied to\n",
    "the LR image after which a *sub-pixel convolutional layer* upscales the LR feature maps to generate the super-resolved image. Additionally, a *deconvolution layer* is also added which is a more generic form of the interpolation filter. More information can be captured when using this additional deconvolution layer. For brevity of content, the mathematically treatment has been abstracted and can be found on the poster.\n",
    "\n",
    "In order to verify that ESPCN could actually outperform the previous super-resolution algorithm, we reproduce this paper by using experiments as following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a part of our reproducibility effort we verify whether the proposed ESPCN can actually outperform previous super-resolution models. \n",
    "\n",
    "- We validate the proposed approach using images and videos from publicly available benchmark datasets.\n",
    "- As an enhancement we propose to use **GELU** as an activation function for the ESPCN model.\n",
    "- We also develop a **video Super-Resolution** pipeline from scratch (not a part of the previous code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image datasets used for evaluation and ablation studies are publicly available, benchmark datasets. \n",
    "1. Training data: \n",
    "    - The Timofte dataset, which contains 91-images.\n",
    "2. Three evaluation datasets:\n",
    "    - Set5 Images\n",
    "    - Set14 Images\n",
    "    - BSD500 Images\n",
    "    \n",
    "Our expermental results are captured in the following sections.\n",
    "\n",
    "As for video experiments, in the paper, the author uses publicly available **Xiph** dataset. As video super-resolution was not part of the reprocibility task, we decided to use publically available **4K Images** dataset [Link](https://www.kaggle.com/evgeniumakov/images4k) to train our model. We present some images from the video-SR results obtained through our experiments with this model.\n",
    " \n",
    "According to the paper, the author ran the experiments on a **K2 GPU** while in our case (*in a bid to obtain standardized results*) we used the **Tesla K80 GPU** available through **Google Colab**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The experiment involves two important steps,\n",
    "1. Generating low-resolution images from the given data. (HR space -> LR space)\n",
    "    - We first perform bicubic re-sampling of the images which are downscaled by a factor of 3 ( `scale` parameter),\n",
    "    - Followed by 17x17 pixel sub-sampling of the original HR images.\n",
    "2. Eventually, the model applies a periodic-shuffling operation these low-res images (subsamples of original HR images) to train and evaluate the model.\n",
    "\n",
    "*Additionally, we work with the **Y-channel (luminance)** as it is most effectively observed.*\n",
    "\n",
    "#### Data Prepearation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prepare(args):\n",
    "    h5_file = h5py.File(args.output_path, 'w')\n",
    "\n",
    "    lr_patches = []\n",
    "    hr_patches = []\n",
    "\n",
    "    for image_path in sorted(glob.glob('{}/*'.format(args.images_dir))):\n",
    "        hr = pil_image.open(image_path).convert('RGB')\n",
    "        hr_width = (hr.width // args.scale) * args.scale\n",
    "        hr_height = (hr.height // args.scale) * args.scale\n",
    "        hr = hr.resize((hr_width, hr_height), resample=pil_image.BICUBIC)\n",
    "        lr = hr.resize((hr_width // args.scale, hr_height // args.scale), resample=pil_image.BICUBIC)\n",
    "        hr = np.array(hr).astype(np.float32)\n",
    "        lr = np.array(lr).astype(np.float32)\n",
    "        hr = convert_rgb_to_y(hr)\n",
    "        lr = convert_rgb_to_y(lr)\n",
    "\n",
    "        for i in range(0, lr.shape[0] - args.patch_size + 1, args.stride):\n",
    "            for j in range(0, lr.shape[1] - args.patch_size + 1, args.stride):\n",
    "                lr_patches.append(lr[i:i + args.patch_size, j:j + args.patch_size])\n",
    "                hr_patches.append(hr[i * args.scale:i * args.scale + args.patch_size * args.scale, j * args.scale:j * args.scale + args.patch_size * args.scale])\n",
    "\n",
    "    lr_patches = np.array(lr_patches)\n",
    "    hr_patches = np.array(hr_patches)\n",
    "\n",
    "    h5_file.create_dataset('lr', data=lr_patches)\n",
    "    h5_file.create_dataset('hr', data=hr_patches)\n",
    "\n",
    "    h5_file.close()\n",
    "    \n",
    "# The input parameters for data preparation.\n",
    "parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--images-dir', type=str, required=True)\n",
    "    parser.add_argument('--output-path', type=str, required=True)\n",
    "    parser.add_argument('--scale', type=int, default=3)\n",
    "    parser.add_argument('--patch-size', type=int, default=17)\n",
    "    parser.add_argument('--stride', type=int, default=13)\n",
    "    parser.add_argument('--eval', action='store_true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Running the experiment\n",
    "### Network framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This is the design of our ESPCN networks, including the intialization \n",
    "weights and forward methods\n",
    "'''\n",
    "import math\n",
    "from torch import nn\n",
    "\n",
    "'''\n",
    "Based on the explanation given in the paper,\n",
    "Number of layers (l) = 3 -> 2 CNN + 1 Sub-pixel\n",
    "Kernel Input is of the form (f_i, n_i) where (5,64) -> (3, 32) -> 3 \n",
    "''' \n",
    "\n",
    "class ESPCN(nn.Module):\n",
    "    def __init__(self, scale_factor, num_channels=1):\n",
    "        super(ESPCN, self).__init__()\n",
    "        self.first_part = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, 64, kernel_size=5, padding=5//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=3//2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.last_part = nn.Sequential(\n",
    "            nn.Conv2d(32, num_channels * (scale_factor ** 2), kernel_size=3, padding=3 // 2),\n",
    "            nn.PixelShuffle(scale_factor)\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                if m.in_channels == 32:\n",
    "                    nn.init.normal_(m.weight.data, mean=0.0, std=0.001)\n",
    "                    nn.init.zeros_(m.bias.data)\n",
    "                else:\n",
    "                    nn.init.normal_(m.weight.data, mean=0.0, std=math.sqrt(2/(m.out_channels*m.weight.data[0][0].numel())))\n",
    "                    nn.init.zeros_(m.bias.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_part(x)\n",
    "        x = self.last_part(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our chosen hyperparameters are as follows:\n",
    "\n",
    "| Hyper-parameters | Value |\n",
    "| :--- | ------ |\n",
    "| `Scale` | **3** |\n",
    "| `learning rate` | **1e-3** |\n",
    "| `batch-size` | **16** |\n",
    "| `number of epochs` | **200** |\n",
    "| `number of workers` | **8** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Image super resolution\n",
    "#### Training\n",
    "The code for training of the ESPCN model is as following.\n",
    "\n",
    "For training, the dataset we used including _91 images dataset_ and also _4k image dataset_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import ESPCN\n",
    "from datasets import TrainDataset, EvalDataset\n",
    "from utils import AverageMeter, calc_psnr\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train-file', type=str, required=True)\n",
    "    parser.add_argument('--eval-file', type=str, required=True)\n",
    "    parser.add_argument('--outputs-dir', type=str, required=True)\n",
    "    parser.add_argument('--weights-file', type=str)\n",
    "    parser.add_argument('--scale', type=int, default=3)\n",
    "    parser.add_argument('--lr', type=float, default=1e-3)\n",
    "    parser.add_argument('--batch-size', type=int, default=16)\n",
    "    parser.add_argument('--num-epochs', type=int, default=200)\n",
    "    parser.add_argument('--num-workers', type=int, default=8)\n",
    "    parser.add_argument('--seed', type=int, default=123)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    args.outputs_dir = os.path.join(args.outputs_dir, 'x{}'.format(args.scale))\n",
    "\n",
    "    if not os.path.exists(args.outputs_dir):\n",
    "        os.makedirs(args.outputs_dir)\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    model = ESPCN(scale_factor=args.scale).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': model.first_part.parameters()},\n",
    "        {'params': model.last_part.parameters(), 'lr': args.lr * 0.1}\n",
    "    ], lr=args.lr)\n",
    "\n",
    "    train_dataset = TrainDataset(args.train_file)\n",
    "    train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                                  batch_size=args.batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=args.num_workers,\n",
    "                                  pin_memory=True)\n",
    "    eval_dataset = EvalDataset(args.eval_file)\n",
    "    eval_dataloader = DataLoader(dataset=eval_dataset, batch_size=1)\n",
    "\n",
    "    best_weights = copy.deepcopy(model.state_dict())\n",
    "    best_epoch = 0\n",
    "    best_psnr = 0.0\n",
    "\n",
    "    for epoch in range(args.num_epochs):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = args.lr * (0.1 ** (epoch // int(args.num_epochs * 0.8)))\n",
    "\n",
    "        model.train()\n",
    "        epoch_losses = AverageMeter()\n",
    "\n",
    "        with tqdm(total=(len(train_dataset) - len(train_dataset) % args.batch_size), ncols=80) as t:\n",
    "            t.set_description('epoch: {}/{}'.format(epoch, args.num_epochs - 1))\n",
    "\n",
    "            for data in train_dataloader:\n",
    "                inputs, labels = data\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                preds = model(inputs)\n",
    "\n",
    "                loss = criterion(preds, labels)\n",
    "\n",
    "                epoch_losses.update(loss.item(), len(inputs))\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                t.set_postfix(loss='{:.6f}'.format(epoch_losses.avg))\n",
    "                t.update(len(inputs))\n",
    "\n",
    "        torch.save(model.state_dict(), os.path.join(args.outputs_dir, 'epoch_{}.pth'.format(epoch)))\n",
    "\n",
    "        model.eval()\n",
    "        epoch_psnr = AverageMeter()\n",
    "\n",
    "        for data in eval_dataloader:\n",
    "            inputs, labels = data\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds = model(inputs).clamp(0.0, 1.0)\n",
    "\n",
    "            epoch_psnr.update(calc_psnr(preds, labels), len(inputs))\n",
    "\n",
    "        print('eval psnr: {:.2f}'.format(epoch_psnr.avg))\n",
    "\n",
    "        if epoch_psnr.avg > best_psnr:\n",
    "            best_epoch = epoch\n",
    "            best_psnr = epoch_psnr.avg\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print('best epoch: {}, psnr: {:.2f}'.format(best_epoch, best_psnr))\n",
    "    torch.save(best_weights, os.path.join(args.outputs_dir, 'best.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the open source code, we made some improvements on the network ourselves. And by changing the network structure, we could achieve better results compared to before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Testing\n",
    "The code for testing of the ESPCN model is as following.\n",
    "\n",
    "For testing, the dataset we used including _Set5, Set14, BSD500_ and also _4k image dataset_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import PIL.Image as pil_image\n",
    "\n",
    "from models import ESPCN\n",
    "from utils import convert_ycbcr_to_rgb, preprocess, calc_psnr\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--weights-file', type=str, required=True)\n",
    "    parser.add_argument('--image-file', type=str, required=True)\n",
    "    parser.add_argument('--scale', type=int, default=3)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "def test(args):\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = ESPCN(scale_factor=args.scale).to(device)\n",
    "\n",
    "    state_dict = model.state_dict()\n",
    "    for n, p in torch.load(args.weights_file, map_location=lambda storage, loc: storage).items():\n",
    "        if n in state_dict.keys():\n",
    "            state_dict[n].copy_(p)\n",
    "        else:\n",
    "            raise KeyError(n)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    image = pil_image.open(args.image_file).convert('RGB')\n",
    "\n",
    "    image_width = (image.width // args.scale) * args.scale\n",
    "    image_height = (image.height // args.scale) * args.scale\n",
    "\n",
    "    hr = image.resize((image_width, image_height), resample=pil_image.BICUBIC)\n",
    "    lr = hr.resize((hr.width // args.scale, hr.height // args.scale), resample=pil_image.BICUBIC)\n",
    "    bicubic = lr.resize((lr.width * args.scale, lr.height * args.scale), resample=pil_image.BICUBIC)\n",
    "    bicubic.save(args.image_file.replace('.', '_bicubic_x{}.'.format(args.scale)))\n",
    "\n",
    "    lr, _ = preprocess(lr, device)\n",
    "    hr, _ = preprocess(hr, device)\n",
    "    _, ycbcr = preprocess(bicubic, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model(lr).clamp(0.0, 1.0)\n",
    "\n",
    "    psnr = calc_psnr(hr, preds)\n",
    "    print('PSNR: {:.2f}'.format(psnr))\n",
    "\n",
    "    preds = preds.mul(255.0).cpu().numpy().squeeze(0).squeeze(0)\n",
    "\n",
    "    output = np.array([preds, ycbcr[..., 1], ycbcr[..., 2]]).transpose([1, 2, 0])\n",
    "    output = np.clip(convert_ycbcr_to_rgb(output), 0.0, 255.0).astype(np.uint8)\n",
    "    output = pil_image.fromarray(output)\n",
    "    output.save(args.image_file.replace('.', '_espcn_x{}.'.format(args.scale)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image SR reproducibility results \n",
    "\n",
    "Based on the train and test set, we get a table of performance as following.\n",
    "\n",
    "|_Dataset_ | _Scale_ | _relu_ | _tanh_ | _gelu_ | _paper_ |\n",
    "|----------|---------|--------|--------|--------|---------|\n",
    "| ` Set5`  |   3   |**33.13**|32.88|32.99|33.00|\n",
    "| ` Set14`  |   3   |**29.49**|29.33|29.40|29.42|\n",
    "| ` BSD500`  |   3   |**28.87**|28.69|28.77|28.62|\n",
    "| ` 4K(test)`  |   3   |43.61| |46.25| |\n",
    "\n",
    "* **Higher PSNR** values were achieved across the board as compared to the results posited in the paper.\n",
    "\n",
    "* Additionally, it should be noted that the model taken from the paper is trained on ImageNet (~50,000 Images) whereas our experimental results were obtained using the 91-Images dataset which is significantly smaller.\n",
    "\n",
    "We compare the results of ESPCN with Bicubic, and it can be clearly seen that ESPCN could obtain better results compared to Bicubic.\n",
    "\n",
    "---\n",
    "![Comparision between ESPCN and bicubic](thumbnails/result.jpg)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning\n",
    "\n",
    "In order to understand the effects and eventually tune the hyperparameters for our model, we carried out several ablation studies as follows.\n",
    "\n",
    "<table><tr><td><img src=\"thumbnails/activations.png\" alt=\"Drawing\" style=\"width: 400px;\"/></td><td><img src=\"thumbnails/psnrbatch.jpeg\" alt=\"Drawing\" style=\"width: 400px;\"/>></td></tr></table>\n",
    "<table><tr><td><img src=\"thumbnails/psnrlearningrate.jpeg\" alt=\"Drawing\" style=\"width: 400px;\"/></td><td><img src=\"thumbnails/psnrscale.jpeg\" alt=\"Drawing\" style=\"width: 400px;\"/>></td></tr></table>\n",
    "\n",
    "From the experiments, we posit the following results:\n",
    "* We observe optimal performance for `batch-size = 4`.\n",
    "* The performance (PSNR value) drops significantly for `learning rate > 0.08`.\n",
    "* `scale = 3` gives optimal performance i.e. the highest PSNR value (but it seems to be quite data dependent).\n",
    "* Observed performance of activation functions based on PSNR scores: `ReLU > GELU > Tanh`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video super resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the image super resolution, we build a pipeline for video super resolution.\n",
    "\n",
    "* Video pipeline takes in 1080p video file as input.\n",
    "* Breaks down the video into frames (images) by sampling at 30 frames per second.\n",
    "* Predicts each frame 4k super-resolution according to the trained model and combines the frames.\n",
    "* Potential Enhancement: live video conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter setting for video SR are as follows:\n",
    "\n",
    "| Hyper-parameters | Value |\n",
    "| :--- | ------ |\n",
    "| `Scale` | **4** |\n",
    "| `learning rate` | **1e-3** |\n",
    "| `batch-size` | **16** |\n",
    "| `number of epochs` | **200** |\n",
    "| `number of workers` | **8** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Results\n",
    "We present the images (below) as a the result of video super resolution. We conclude that it does a pretty good job of sharpening the images.\n",
    "\n",
    "<img src=\"thumbnails/superResVs1080P.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<img src=\"thumbnails/heyenaVs.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<img src=\"thumbnails/elephant.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}