{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network (ESPCN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Reproducibility Effort\n",
    "\n",
    "In this blog-post, we compile our efforts to reproduce the posited results from table 1 of the paper [Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network](https://arxiv.org/abs/1609.05158) by Shi et al. Building upon the code effort by Jeffrey Yeo (yjn870) [[Github repo](https://github.com/yjn870/ESPCN-pytorch)], we made certain improvements and additionally reproduced the experiments for 4K and video from the paper, which were not a part of Yeo's previous work. We present our reproducibility effort as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Super-Resolution\n",
    "\n",
    "In deep learning, especially in computer vision and related research, many researchers have been focusing on the *ill-posed* image **super-resolution** (SR) problem, which involves upscaling a low resolution image into a high resolution space. This technique could be used to restore image quality and could also enhance general image processing. In fields like facial recognition, medical imaging and even satellite imaging, super-resolution has been widely applied. Its broad use-case has allowed it to become one of the most popular topics in Computer Vision. \n",
    "\n",
    "## Why do we need *ESPCN*?\n",
    " \n",
    "In previous SR models like [SRCNN](https://arxiv.org/abs/1501.00092) and [TNRD](https://arxiv.org/pdf/1508.02848), the super-resolution operation was carried out in the high-resolution (HR) space. The sub-optimality and additional computational complexity of this approach motivated the development of **Efficient Sub-Pixel Convolution layer** (ESPCN). Upscaling the resolution of low-resolution (LR) images before the image enhancement step is a major pain-point behind the increase in computational complexity. In convolutional networks, this complexity severly influences the speed of the implementation. Moreover, traditional interpolation methods used in super-resolution (previously) fail to capture additional (crucial) information required to solve this ill-posed problem!\n",
    "\n",
    "Notably, in the proposed ESPCN, feature maps are extracted in the LR space and upscaling of images (from LR to HR) is performed in the final layer of the network. Super-resolving HR data from LR feature-maps in this way greatly increases the efficiency of the SR model as most of the computation is done in the smaller LR space. What's more? in ESPCN, no explicit interpolation filter is used which means that the network is able to implicitly learn the processing necesaary for super-resolution. It is thus able to learn a better mapping from low-resolution image to high-resolution image compared to using a single fixed filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ESPCN with 2 CNN layers and 1 Sub-Pixel Convolution Layer](thumbnails/fig1.png)\n",
    "\n",
    "##### ESPCN with 2 CNN layers and 1 Sub-Pixel Convolution Layer\n",
    "In the proposed architecture, firstly, an *L* layer convolutional neural network is directly applied to\n",
    "the LR image after which a *sub-pixel convolutional layer* upscales the LR feature maps to generate the super-resolved image. Additionally, a *deconvolution layer* is also added which is a more generic form of the interpolation filter. More information can be captured when using this additional deconvolution layer.\n",
    "\n",
    "In order to verify that ESPCN could actually outperform the previous super-resolution algorithm, we reproduce this paper by using experiments as following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a part of our reproducibility effort we verify whether the proposed ESPCN can actually outperform previous super-resolution models. \n",
    "\n",
    "- We validate the proposed approach using images and videos from publicly available benchmark datasets.\n",
    "- As an enhancement we propose to use **GELU** as an activation function for the ESPCN model.\n",
    "- We also develop a **video Super-Resolution** pipeline from scratch (not a part of the previous code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two image datasets used for evaluation are public available benchmark datasets. The first one is the Timofte dataset, which contains 91 training images and two test dataset. The second one is 50,000 randomly selected images from ImageNet for the training.\n",
    "\n",
    "As for video experiments, in the paper, the author uses publicly available Xiph database. *INPUT OUR VIDEO DATASET HERE*\n",
    "\n",
    "According to the paper, the author ran the experiment on a K2 GPU while in our cases, we ran our experiment on our local computer, which is *INPUT YOUR COMPUTER GPU HERE* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the experiment\n",
    "### Network framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is the design of our ESPCN networks, including the intialization \n",
    "weights and forward methods\n",
    "'''\n",
    "import math\n",
    "from torch import nn\n",
    "\n",
    "'''\n",
    "Based on the explanation given in the paper,\n",
    "Number of layers (l) = 3 -> 2 CNN + 1 Sub-pixel\n",
    "Kernel Input is of the form (f_i, n_i) where (5,64) -> (3, 32) -> 3 \n",
    "''' \n",
    "\n",
    "class ESPCN(nn.Module):\n",
    "    def __init__(self, scale_factor, num_channels=1):\n",
    "        super(ESPCN, self).__init__()\n",
    "        self.first_part = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, 64, kernel_size=5, padding=5//2),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=3//2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.last_part = nn.Sequential(\n",
    "            nn.Conv2d(32, num_channels * (scale_factor ** 2), kernel_size=3, padding=3 // 2),\n",
    "            nn.PixelShuffle(scale_factor)\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                if m.in_channels == 32:\n",
    "                    nn.init.normal_(m.weight.data, mean=0.0, std=0.001)\n",
    "                    nn.init.zeros_(m.bias.data)\n",
    "                else:\n",
    "                    nn.init.normal_(m.weight.data, mean=0.0, std=math.sqrt(2/(m.out_channels*m.weight.data[0][0].numel())))\n",
    "                    nn.init.zeros_(m.bias.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_part(x)\n",
    "        x = self.last_part(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Settings\n",
    "\n",
    "Our chosen hyperparameters are as follows:\n",
    "\n",
    "| Hyper-parameters | Value |\n",
    "| :--- | ------ |\n",
    "| `Scale` | **3** |\n",
    "| `learning rate` | **1e-3** |\n",
    "| `batch-size` | **16** |\n",
    "| `number of epochs` | **200** |\n",
    "| `number of workers` | **8** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image super resolution\n",
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import ESPCN\n",
    "from datasets import TrainDataset, EvalDataset\n",
    "from utils import AverageMeter, calc_psnr\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train-file', type=str, required=True)\n",
    "    parser.add_argument('--eval-file', type=str, required=True)\n",
    "    parser.add_argument('--outputs-dir', type=str, required=True)\n",
    "    parser.add_argument('--weights-file', type=str)\n",
    "    parser.add_argument('--scale', type=int, default=3)\n",
    "    parser.add_argument('--lr', type=float, default=1e-3)\n",
    "    parser.add_argument('--batch-size', type=int, default=16)\n",
    "    parser.add_argument('--num-epochs', type=int, default=200)\n",
    "    parser.add_argument('--num-workers', type=int, default=8)\n",
    "    parser.add_argument('--seed', type=int, default=123)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    args.outputs_dir = os.path.join(args.outputs_dir, 'x{}'.format(args.scale))\n",
    "\n",
    "    if not os.path.exists(args.outputs_dir):\n",
    "        os.makedirs(args.outputs_dir)\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    model = ESPCN(scale_factor=args.scale).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': model.first_part.parameters()},\n",
    "        {'params': model.last_part.parameters(), 'lr': args.lr * 0.1}\n",
    "    ], lr=args.lr)\n",
    "\n",
    "    train_dataset = TrainDataset(args.train_file)\n",
    "    train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                                  batch_size=args.batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=args.num_workers,\n",
    "                                  pin_memory=True)\n",
    "    eval_dataset = EvalDataset(args.eval_file)\n",
    "    eval_dataloader = DataLoader(dataset=eval_dataset, batch_size=1)\n",
    "\n",
    "    best_weights = copy.deepcopy(model.state_dict())\n",
    "    best_epoch = 0\n",
    "    best_psnr = 0.0\n",
    "\n",
    "    for epoch in range(args.num_epochs):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = args.lr * (0.1 ** (epoch // int(args.num_epochs * 0.8)))\n",
    "\n",
    "        model.train()\n",
    "        epoch_losses = AverageMeter()\n",
    "\n",
    "        with tqdm(total=(len(train_dataset) - len(train_dataset) % args.batch_size), ncols=80) as t:\n",
    "            t.set_description('epoch: {}/{}'.format(epoch, args.num_epochs - 1))\n",
    "\n",
    "            for data in train_dataloader:\n",
    "                inputs, labels = data\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                preds = model(inputs)\n",
    "\n",
    "                loss = criterion(preds, labels)\n",
    "\n",
    "                epoch_losses.update(loss.item(), len(inputs))\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                t.set_postfix(loss='{:.6f}'.format(epoch_losses.avg))\n",
    "                t.update(len(inputs))\n",
    "\n",
    "        torch.save(model.state_dict(), os.path.join(args.outputs_dir, 'epoch_{}.pth'.format(epoch)))\n",
    "\n",
    "        model.eval()\n",
    "        epoch_psnr = AverageMeter()\n",
    "\n",
    "        for data in eval_dataloader:\n",
    "            inputs, labels = data\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds = model(inputs).clamp(0.0, 1.0)\n",
    "\n",
    "            epoch_psnr.update(calc_psnr(preds, labels), len(inputs))\n",
    "\n",
    "        print('eval psnr: {:.2f}'.format(epoch_psnr.avg))\n",
    "\n",
    "        if epoch_psnr.avg > best_psnr:\n",
    "            best_epoch = epoch\n",
    "            best_psnr = epoch_psnr.avg\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print('best epoch: {}, psnr: {:.2f}'.format(best_epoch, best_psnr))\n",
    "    torch.save(best_weights, os.path.join(args.outputs_dir, 'best.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the open source code, we made some improvements on the network ourselves. And by changing the network structure, we could achieve better results compared to before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image SR reproducibility results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional note about GELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video super resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
